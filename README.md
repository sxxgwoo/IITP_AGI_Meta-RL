# Meta-RL

# ğŸŒŸ DynaMITE-RL & HLT-Dynamite-RL

ì´ ë ˆí¬ì§€í† ë¦¬ëŠ” **DynaMITE-RL(NeurIPS 2024)** ì•Œê³ ë¦¬ì¦˜ì˜ GridWorld êµ¬í˜„ê³¼,
ì´ë¥¼ í™•ì¥í•´ ìƒˆë¡œìš´ latent êµ¬ì¡°ë¥¼ ì‹¤í—˜í•˜ëŠ” **HLT-DynaMITE-RL ë²„ì „**ì„ í¬í•¨í•˜ê³  ìˆë‹¤.
í•µì‹¬ ì›ë¦¬ëŠ” ëª¨ë‘ ë…¼ë¬¸ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤. 

---

# ğŸ“ 1. Directory Structure

```
meta_rl/
â”‚
â”œâ”€â”€ dynamite_rl/                # ì› ë…¼ë¬¸ êµ¬ì¡°ë¥¼ ë”°ë¥´ëŠ” DynaMITE-RL êµ¬í˜„
â”‚   â”œâ”€â”€ checkpoints/            # ì €ì¥ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜
â”‚   â”œâ”€â”€ logs/                   # í•™ìŠµ ê³¼ì • ë¡œê·¸
â”‚   â”œâ”€â”€ agent.py                # PPO ì—ì´ì „íŠ¸ + latent belief update
â”‚   â”œâ”€â”€ config.py               # ì‹¤í—˜ ì„¤ì • (learning rate, latent dim ë“±)
â”‚   â”œâ”€â”€ envs.py                 # GridWorld ê¸°ë°˜ DLCMDP í™˜ê²½
â”‚   â”œâ”€â”€ main.py                 # DynaMITE-RL í•™ìŠµ ì „ì²´ íŒŒì´í”„ë¼ì¸
â”‚   â”œâ”€â”€ models.py               # VAE encoder/decoder, policy/value network
â”‚   â””â”€â”€ train_YYYYMMDD_xxxxxx.log
â”‚
â”œâ”€â”€ hlt_dynamite_rl/            # HLT-DynaMITE-RL: latent êµ¬ì¡° í™•ì¥ ì‹¤í—˜
â”‚   â”œâ”€â”€ checkpoints_hlt/        # HLT ì‹¤í—˜ìš© ëª¨ë¸ ì €ì¥
â”‚   â”œâ”€â”€ graph/                  # í•™ìŠµê³¡ì„ , best policy evaluation ê·¸ë˜í”„
â”‚   â”œâ”€â”€ logs/                   # HLT ë²„ì „ ë¡œê·¸
â”‚   â”œâ”€â”€ agent.py                # HLT-DynaMITE-RL agent (top/mid latent ì§€ì›)
â”‚   â”œâ”€â”€ config.py               # HLT ì‹¤í—˜ ì„¤ì •
â”‚   â”œâ”€â”€ envs.py                 # ë™ì¼ GridWorld í™˜ê²½ (HLT ìš©)
â”‚   â”œâ”€â”€ main.py                 # HLT-DynaMITE-RL í•™ìŠµ íŒŒì´í”„ë¼ì¸
â”‚   â””â”€â”€ models.py               # top/mid latent êµ¬ì¡° ëª¨ë¸ ì •ì˜
â”‚
â””â”€â”€ README.md
```

---

# ğŸ” 2. What is Implemented?

## âœ” DynaMITE-RL (ë…¼ë¬¸ ì›í˜• ì¬í˜„)

* Session ë‹¨ìœ„ë¡œ ë³€í•˜ëŠ” latent context (DLCMDP)
* Variational inference ê¸°ë°˜ latent posterior ì—…ë°ì´íŠ¸
* Consistency loss ì ìš©
* Previous posterior â†’ next prior (latent belief conditioning)
* Session maskingìœ¼ë¡œ reconstruction ì•ˆì •í™”

## âœ” HLT-DynaMITE-RL

* **v1:** top latent only
* **v2:** mid-latentê°€ top latent í•™ìŠµì„ ë³´ì¡°
* latent disentanglement ë° RL ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜

---

# ğŸ§  3. Key Files Overview

### `envs.py`

* 5Ã—5 GridWorld
* ë‘ goal ì¤‘ í•˜ë‚˜ë§Œ sessionë§ˆë‹¤ +1
* DLCMDP termination(p_switch) ì ìš©

### `models.py`

* GRU ê¸°ë°˜ encoder
* Gaussian posterior ( q(z|\tau) )
* session termination head
* state/reward decoder
* PPO actor/critic

### `agent.py`

* rollout ì¤‘ posterior ì—…ë°ì´íŠ¸
* consistency + masked ELBO ê³„ì‚°
* PPO advantage ê³„ì‚°

### `main.py`

* í•™ìŠµ ì „ì²´ íŒŒì´í”„ë¼ì¸
* rollout â†’ VAE update â†’ PPO update
* checkpoint & logging

---

# ğŸš€ 4. How to Run

### DynaMITE-RL

```
cd meta_rl/dynamite_rl
python main.py
```

### HLT-DynaMITE-RL

```
cd meta_rl/hlt_dynamite_rl
python main.py
```

---

# ğŸ“ˆ 5. Logs & Graphs

* `logs/` : í•™ìŠµ ë¡œê·¸
* `checkpoints*/` : ëª¨ë¸ ê°€ì¤‘ì¹˜
* `graph/` : HLT-DynaMITE-RL í•™ìŠµê³¡ì„  ë° best-policy reward ê³¡ì„ 

  * mean Â± std shading í¬í•¨

ê·¸ë˜í”„ ì œëª© ì˜ˆì‹œ:
**Best Policy Evaluation Reward Over Training**

---

# âš™ï¸ 6. Configuration

ê° config.pyì—ì„œ ì„¤ì • ê°€ëŠ¥:

* latent dimension
* learning rate
* PPO hyperparameters
* p_switch
* training steps
* seed

---

# ğŸ“š 7. Reference

ë³¸ êµ¬í˜„ì€ ë‹¤ìŒ ë…¼ë¬¸ì— ê¸°ë°˜í•œë‹¤:
**DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement Learning (NeurIPS 2024)**
ì „ì²´ ì•Œê³ ë¦¬ì¦˜Â·ìˆ˜ì‹Â·ELBO ë„ì‹ì€ PDF ì°¸ì¡°. 


